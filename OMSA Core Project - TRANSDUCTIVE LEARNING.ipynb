{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import scipy\n",
    "from sklearn import linear_model, metrics, pipeline, preprocessing\n",
    "import imblearn\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fir the y column: 1 = confirmed fraud, 0 = passed OTP (good customer), -1 = unknown status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "1. drop duplicates from the dummy variables\n",
    "2. use new data\n",
    "3. setup dummy variables as dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reading full df\n",
    "#df = pd.read_csv('omsa_project_cleaned_data.csv')\n",
    "df = scipy.sparse.load_npz('omsa_project_cleaned_data.npz').tocsr()\n",
    "y = pd.read_csv('omsa_project_cleaned_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values.ravel()\n",
    "pre_pseudo_labels = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ssl_labeled = (y != -1).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Setup\n",
    "-test model on full dataset, will probably do even better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_change_step = 100\n",
    "limit_master = 0.95\n",
    "step_master = 0.05\n",
    "iterations_max = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = LinearSVC(dual=False, )\n",
    "clf = SGDClassifier(warm_start=True, loss='modified_huber', tol=1e-3, max_iter=1000) #warm_start is import for our setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start timer\n",
    "start_time = time.time()\n",
    "\n",
    "iterations = 1\n",
    "limit = limit_master\n",
    "step = step_master\n",
    "y = pre_pseudo_labels.copy()\n",
    "\n",
    "accuracy = []\n",
    "accuracy_orig_data = []\n",
    "remain_unlabeled_obs = [(y == -1).sum()]\n",
    "converged = False\n",
    "while iterations < iterations_max and (not converged):\n",
    "    y0 = y.copy()\n",
    "    \n",
    "    #restart the filters based on what we have labeled now\n",
    "    pseudo_assigned = (y != -1).ravel() #update to reflect newly assigned labels that don't require a label assignment now\n",
    "    pseudo_missing = np.invert(pseudo_assigned)\n",
    "    #restart the split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "   \n",
    "    #fit on most recent training data\n",
    "    clf.fit(x_train, y_train)\n",
    "    probs = clf.predict_proba(df)\n",
    "    \n",
    "    #save accuracy\n",
    "    y_test_preds = clf.predict(x_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    \n",
    "    #accuracy on just the original labeled points\n",
    "    accuracy_orig_data.append(accuracy_score(y_test[pre_ssl_labeled], y_test_preds[pre_ssl_labeled]))\n",
    "    \n",
    "    \n",
    "    #filter for if an element met the min probability in any column\n",
    "    min_prob_met = probs > limit\n",
    "    \n",
    "    #filter out rows where the min probability is not met on any obs for either class\n",
    "    any_strong_prediction = min_prob_met.sum(axis=1) != 0\n",
    "    \n",
    "    #will give us which class had the highest probability (combined with any_strong_prediction will get us where we want to be)\n",
    "    highest_prob_class = np.argmax(probs, axis=1)\n",
    "    \n",
    "    #save predictions of missing label data points and where we had a strong enough prediction for either class\n",
    "    y[pseudo_missing & any_strong_prediction] = highest_prob_class[pseudo_missing & any_strong_prediction]\n",
    "    \n",
    "    \n",
    "    #how are we doing now with accuracy and these new labels\n",
    "    #y_test_preds = clf.predict(x_test)\n",
    "    #accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    remain_unlabeled_obs.append((y == -1).sum())\n",
    "    \n",
    "    if np.array_equal(y0,y):\n",
    "        limit = limit - step\n",
    "    if (remain_unlabeled_obs[-2] - remain_unlabeled_obs[-1]) < min_change_step:\n",
    "        limit = limit - step\n",
    "    if remain_unlabeled_obs[-1] == 0:\n",
    "        converged = True\n",
    "        \n",
    "    #print((y == -1).sum()) #obs still to update after this iteration\n",
    "    #print(limit)\n",
    "    iterations +=1\n",
    "print(iterations)    \n",
    "#final accuracy\n",
    "pseudo_assigned = (y != -1).ravel()\n",
    "pseudo_missing = np.invert(pseudo_assigned)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "clf.fit(x_train, y_train)\n",
    "probs_csvm = clf.predict_proba(df)\n",
    "y_test_preds = clf.predict(x_test)\n",
    "accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "#end timer\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#start timer\n",
    "start_time = time.time()\n",
    "\n",
    "iterations = 1\n",
    "limit = 0.95\n",
    "step = 0.05\n",
    "y = pre_pseudo_labels.copy()\n",
    "\n",
    "accuracy = []\n",
    "remain_unlabeled_obs = [(y == -1).sum()]\n",
    "converged = False\n",
    "while iterations < 100 and (not converged):\n",
    "    y0 = y.copy()\n",
    "    \n",
    "    #restart the filters based on what we have labeled now\n",
    "    pseudo_assigned = (y != -1).ravel()\n",
    "    pseudo_missing = np.invert(pseudo_assigned)\n",
    "    #restart the split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "   \n",
    "    #fit on most recent training data\n",
    "    clf.fit(x_train, y_train)\n",
    "    probs = clf.predict_proba(df)\n",
    "    \n",
    "    #save accuracy\n",
    "    y_test_preds = clf.predict(x_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    \n",
    "    \n",
    "    #filter for if an element met the min probability in any column\n",
    "    min_prob_met = probs > limit\n",
    "    \n",
    "    #filter out rows where the min probability is not met on any obs for either class\n",
    "    any_strong_prediction = min_prob_met.sum(axis=1) != 0\n",
    "    \n",
    "    #will give us which class had the highest probability (combined with any_strong_prediction will get us where we want to be)\n",
    "    highest_prob_class = np.argmax(probs, axis=1)\n",
    "    \n",
    "    #save predictions of missing label data points and where we had a strong enough prediction for either class\n",
    "    y[pseudo_missing & any_strong_prediction] = highest_prob_class[pseudo_missing & any_strong_prediction]\n",
    "    \n",
    "    \n",
    "    #how are we doing now with accuracy and these new labels\n",
    "    #y_test_preds = clf.predict(x_test)\n",
    "    #accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    remain_unlabeled_obs.append((y == -1).sum())\n",
    "    \n",
    "    if np.array_equal(y0,y):\n",
    "        limit = limit - step\n",
    "    if (remain_unlabeled_obs[-2] - remain_unlabeled_obs[-1]) < 500:\n",
    "        limit = limit - step\n",
    "    if remain_unlabeled_obs[-1] == 0:\n",
    "        converged = True\n",
    "        \n",
    "    #print((y == -1).sum()) #obs still to update after this iteration\n",
    "    #print(limit)\n",
    "    iterations +=1\n",
    "print(iterations)    \n",
    "#final accuracy\n",
    "pseudo_assigned = (y != -1).ravel()\n",
    "pseudo_missing = np.invert(pseudo_assigned)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "clf.fit(x_train, y_train)\n",
    "probs_isvm = clf.predict_proba(df)\n",
    "y_test_preds = clf.predict(x_test)\n",
    "accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,iterations), accuracy)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance with Semi Supervised Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_labels_imputed = abs(remain_unlabeled_obs - remain_unlabeled_obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missing_labels_imputed, accuracy)\n",
    "plt.xlabel('# Missing Labels Imputed')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance with Semi Supervised Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_labels_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pre_pseudo_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save values\n",
    "accuracy_isvm = accuracy.copy()\n",
    "iterations_isvm = iterations\n",
    "missing_labels_imputed_isvm = missing_labels_imputed.copy()\n",
    "y_isvm = y.copy()\n",
    "accuracy_orig_data_isvm = accuracy_orig_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster-then-label (fuzzy c-means & svm approach)\n",
    "1. visualize clusters with TSNE\n",
    "2. fuzzy c-means clusters\n",
    "3. analysis of cluster separability\n",
    "4. Assign labels\n",
    "5. SVM and look at accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install fuzzy-c-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skfuzzy as fuzz\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing and utilizing PCA before KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    \n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    \n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "            \n",
    "    # Return the number of components\n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD()\n",
    "tsvd.fit(df)\n",
    "optimal_comp = select_n_components(tsvd.explained_variance_ratio_, 0.995)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components = optimal_comp)\n",
    "df_pca = tsvd.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a sample to determine the optimal K value here for KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(list(range(0,df.shape[0])), 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
    "\n",
    "# function returns WSS score for k values from 1 to kmax\n",
    "def calculate_WSS(points, kmax):\n",
    "  sse = []\n",
    "  for k in range(1, kmax+1):\n",
    "    kmeans = KMeans(n_clusters = k).fit(points)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    pred_clusters = kmeans.predict(points)\n",
    "    curr_sse = 0\n",
    "    \n",
    "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "    for i in range(points.shape[0]):\n",
    "      curr_center = centroids[pred_clusters[i]]\n",
    "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "      \n",
    "    sse.append(curr_sse)\n",
    "  return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " optimal_k = np.argmin(calculate_WSS(df_pca[sample], 10)) +1 #plus 1 for index starts at 1 as min cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our clustering now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_k, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array of the\n",
    "df_cluster = kmeans.fit_transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this model we are changing the X we use. \n",
    "df_orig = df.copy()\n",
    "df = df_cluster.copy()\n",
    "del df_cluster, df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use our new clustering df to run svm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start timer\n",
    "start_time = time.time()\n",
    "\n",
    "iterations = 1\n",
    "limit = limit_master\n",
    "step = step_master\n",
    "y = pre_pseudo_labels.copy()\n",
    "\n",
    "accuracy = []\n",
    "accuracy_orig_data = []\n",
    "remain_unlabeled_obs = [(y == -1).sum()]\n",
    "converged = False\n",
    "while iterations < iterations_max and (not converged):\n",
    "    y0 = y.copy()\n",
    "    \n",
    "    #restart the filters based on what we have labeled now\n",
    "    pseudo_assigned = (y != -1).ravel() #update to reflect newly assigned labels that don't require a label assignment now\n",
    "    pseudo_missing = np.invert(pseudo_assigned)\n",
    "    #restart the split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "   \n",
    "    #fit on most recent training data\n",
    "    clf.fit(x_train, y_train)\n",
    "    probs = clf.predict_proba(df)\n",
    "    \n",
    "    #save accuracy\n",
    "    y_test_preds = clf.predict(x_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    \n",
    "    #accuracy on just the original labeled points\n",
    "    accuracy_orig_data.append(accuracy_score(y_test[pre_ssl_labeled], y_test_preds[pre_ssl_labeled]))\n",
    "    \n",
    "    \n",
    "    #filter for if an element met the min probability in any column\n",
    "    min_prob_met = probs > limit\n",
    "    \n",
    "    #filter out rows where the min probability is not met on any obs for either class\n",
    "    any_strong_prediction = min_prob_met.sum(axis=1) != 0\n",
    "    \n",
    "    #will give us which class had the highest probability (combined with any_strong_prediction will get us where we want to be)\n",
    "    highest_prob_class = np.argmax(probs, axis=1)\n",
    "    \n",
    "    #save predictions of missing label data points and where we had a strong enough prediction for either class\n",
    "    y[pseudo_missing & any_strong_prediction] = highest_prob_class[pseudo_missing & any_strong_prediction]\n",
    "    \n",
    "    \n",
    "    #how are we doing now with accuracy and these new labels\n",
    "    #y_test_preds = clf.predict(x_test)\n",
    "    #accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    remain_unlabeled_obs.append((y == -1).sum())\n",
    "    \n",
    "    if np.array_equal(y0,y):\n",
    "        limit = limit - step\n",
    "    if (remain_unlabeled_obs[-2] - remain_unlabeled_obs[-1]) < min_change_step:\n",
    "        limit = limit - step\n",
    "    if remain_unlabeled_obs[-1] == 0:\n",
    "        converged = True\n",
    "        \n",
    "    #print((y == -1).sum()) #obs still to update after this iteration\n",
    "    #print(limit)\n",
    "    iterations +=1\n",
    "print(iterations)    \n",
    "#final accuracy\n",
    "pseudo_assigned = (y != -1).ravel()\n",
    "pseudo_missing = np.invert(pseudo_assigned)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "clf.fit(x_train, y_train)\n",
    "probs_csvm = clf.predict_proba(df)\n",
    "y_test_preds = clf.predict(x_test)\n",
    "accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "#end timer\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return to normal\n",
    "df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,iterations), accuracy)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance with Semi Supervised Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_labels_imputed = abs(remain_unlabeled_obs - remain_unlabeled_obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missing_labels_imputed, accuracy)\n",
    "plt.xlabel('# Missing Labels Imputed')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance with Semi Supervised Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_labels_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pre_pseudo_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save values\n",
    "accuracy_csvm = accuracy.copy()\n",
    "iterations_csvm = iterations\n",
    "missing_labels_imputed_csvm = missing_labels_imputed.copy()\n",
    "y_csvm = y.copy()\n",
    "accuracy_orig_data_csvm = accuracy_orig_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Label Propagation (Previously TSVM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY IF WE HAVE TIME!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPropagation(kernel = 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset y\n",
    "#y = pre_pseudo_labels.copy()\n",
    "#run model, -1 labels are considered \"unlabeled\"\n",
    "#clf.fit(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start timer\n",
    "start_time = time.time()\n",
    "\n",
    "iterations = 1\n",
    "limit = limit_master\n",
    "step = step_master\n",
    "y = pre_pseudo_labels.copy()\n",
    "\n",
    "accuracy = []\n",
    "accuracy_orig_data = []\n",
    "remain_unlabeled_obs = [(y == -1).sum()]\n",
    "converged = False\n",
    "while iterations < iterations_max and (not converged):\n",
    "    y0 = y.copy()\n",
    "    \n",
    "    #restart the filters based on what we have labeled now\n",
    "    pseudo_assigned = (y != -1).ravel() #update to reflect newly assigned labels that don't require a label assignment now\n",
    "    pseudo_missing = np.invert(pseudo_assigned)\n",
    "    #restart the split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "   \n",
    "    #fit on most recent training data\n",
    "    clf.fit(x_train, y_train)\n",
    "    probs = clf.predict_proba(df)\n",
    "    \n",
    "    #save accuracy\n",
    "    y_test_preds = clf.predict(x_test)\n",
    "    accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    \n",
    "    #accuracy on just the original labeled points\n",
    "    accuracy_orig_data.append(accuracy_score(y_test[pre_ssl_labeled], y_test_preds[pre_ssl_labeled]))\n",
    "    \n",
    "    \n",
    "    #filter for if an element met the min probability in any column\n",
    "    min_prob_met = probs > limit\n",
    "    \n",
    "    #filter out rows where the min probability is not met on any obs for either class\n",
    "    any_strong_prediction = min_prob_met.sum(axis=1) != 0\n",
    "    \n",
    "    #will give us which class had the highest probability (combined with any_strong_prediction will get us where we want to be)\n",
    "    highest_prob_class = np.argmax(probs, axis=1)\n",
    "    \n",
    "    #save predictions of missing label data points and where we had a strong enough prediction for either class\n",
    "    y[pseudo_missing & any_strong_prediction] = highest_prob_class[pseudo_missing & any_strong_prediction]\n",
    "    \n",
    "    \n",
    "    #how are we doing now with accuracy and these new labels\n",
    "    #y_test_preds = clf.predict(x_test)\n",
    "    #accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "    remain_unlabeled_obs.append((y == -1).sum())\n",
    "    \n",
    "    if np.array_equal(y0,y):\n",
    "        limit = limit - step\n",
    "    if (remain_unlabeled_obs[-2] - remain_unlabeled_obs[-1]) < min_change_step:\n",
    "        limit = limit - step\n",
    "    if remain_unlabeled_obs[-1] == 0:\n",
    "        converged = True\n",
    "        \n",
    "    #print((y == -1).sum()) #obs still to update after this iteration\n",
    "    #print(limit)\n",
    "    iterations +=1\n",
    "print(iterations)    \n",
    "#final accuracy\n",
    "pseudo_assigned = (y != -1).ravel()\n",
    "pseudo_missing = np.invert(pseudo_assigned)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[pseudo_assigned], y[pseudo_assigned], test_size=0.15, random_state=30, stratify=y[pseudo_assigned])\n",
    "clf.fit(x_train, y_train)\n",
    "probs_csvm = clf.predict_proba(df)\n",
    "y_test_preds = clf.predict(x_test)\n",
    "accuracy.append(accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "#end timer\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,iterations), accuracy)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance with Semi Supervised Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_labels_imputed = abs(remain_unlabeled_obs - remain_unlabeled_obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missing_labels_imputed, accuracy)\n",
    "plt.xlabel('# Missing Labels Imputed')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance with Semi Supervised Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_labels_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pre_pseudo_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save values\n",
    "accuracy_lp = accuracy.copy()\n",
    "iterations_lp = iterations\n",
    "missing_labels_imputed_lp = missing_labels_imputed.copy()\n",
    "y_lp = y.copy()\n",
    "accuracy_orig_data_lp = accuracy_orig_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,iterations_isvm), accuracy_isvm, label = 'Self Training SVM')\n",
    "plt.plot(range(0,iterations_csvm), accuracy_csvm, label = 'Clustering SVM/cluster-then-label')\n",
    "plt.plot(range(0,iterations_lp), accuracy_lp, label = 'KNN Label Propagation')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance On Updated Labels')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0,iterations_isvm), accuracy_orig_data_isvm, label = 'Self Training SVM')\n",
    "plt.plot(range(0,iterations_csvm), accuracy_orig_data_csvm, label = 'Clustering SVM/cluster-then-label')\n",
    "plt.plot(range(0,iterations_lp), accuracy_orig_data_lp, label = 'KNN Label Propagation')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance On Original Non-Propagated Labels')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missing_labels_imputed_isvm, accuracy_isvm, label = 'Self Training SVM')\n",
    "plt.plot(missing_labels_imputed_csvm, accuracy_csvm, label = 'Clustering SVM/cluster-then-label')\n",
    "plt.plot(missing_labels_imputed_lp, accuracy_lp, label = 'KNN Label Propagation')\n",
    "plt.xlabel('# Missing Labels Imputed')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance On Updated Labels')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(missing_labels_imputed_isvm, accuracy_orig_data_isvm, label = 'Self Training SVM')\n",
    "plt.plot(missing_labels_imputed_csvm, accuracy_orig_data_csvm, label = 'Clustering SVM/cluster-then-label')\n",
    "plt.plot(missing_labels_imputed_lp, accuracy_orig_data_lp, label = 'KNN Label Propagation')\n",
    "plt.xlabel('# Missing Labels Imputed')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Performance On Original Non-Propagated Labels')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Absolute Difference in predicted probability across the different models\n",
    "abs_diff_probabilities_models = np.zeros((3,3))\n",
    "for ii, i in enumerate([probs_isvm, probs_csvm, probs_lp]):\n",
    "    for ji, j in enumerate([probs_isvm, probs_csvm, probs_lp]):\n",
    "        abs_diff_probabilities_models[ii,ji] = abs(i - j).sum()\n",
    "\n",
    "print(abs_diff_probabilities_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum of Squared Difference in Probabilities across classes\n",
    "ssd_classes_and_models = np.zeros((3,1))\n",
    "for ii, i in enumerate([probs_isvm, probs_csvm, probs_lp]):\n",
    "        ssd_classes_and_models[ii] = ((i[:,0] - i[:,1])**2).sum()\n",
    "\n",
    "print(ssd_classes_and_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Probability of classes\n",
    "median_probability_classes = np.zeros((3,2))\n",
    "for ii, i in enumerate([probs_isvm, probs_csvm, probs_lp]):\n",
    "        median_probability_classes[ii,0] = np.median(i[:,0])\n",
    "        median_probability_classes[ii,1] = np.median(i[:,1])\n",
    "\n",
    "print(median_probability_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
